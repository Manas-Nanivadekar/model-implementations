{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82065746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Invalid requirement: 'torch,': Expected end or semicolon (after name and no valid version specifier)\n",
      "    torch,\n",
      "         ^\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install torch, transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff20c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a26db6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/Users/manasnanivadekar/IISc/model-implementations/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.36328125,\n",
       "  'token': 34625,\n",
       "  'token_str': ' sugars',\n",
       "  'sequence': 'Plants create sugars through a process known as photosynthesis.'},\n",
       " {'score': 0.1217041015625,\n",
       "  'token': 1007,\n",
       "  'token_str': ' energy',\n",
       "  'sequence': 'Plants create energy through a process known as photosynthesis.'},\n",
       " {'score': 0.09332275390625,\n",
       "  'token': 11747,\n",
       "  'token_str': ' oxygen',\n",
       "  'sequence': 'Plants create oxygen through a process known as photosynthesis.'},\n",
       " {'score': 0.07501220703125,\n",
       "  'token': 1109,\n",
       "  'token_str': ' light',\n",
       "  'sequence': 'Plants create light through a process known as photosynthesis.'},\n",
       " {'score': 0.056610107421875,\n",
       "  'token': 514,\n",
       "  'token_str': ' water',\n",
       "  'sequence': 'Plants create water through a process known as photosynthesis.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = pipeline(\n",
    "    task=\"fill-mask\",\n",
    "    model=\"FacebookAI/roberta-base\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0\n",
    ")\n",
    "pipeline(\"Plants create <mask> through a process known as photosynthesis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ae0f0",
   "metadata": {},
   "source": [
    "# Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "550fa7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/Users/manasnanivadekar/IISc/model-implementations/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top predictions:\n",
      "1. Bird has\u001b[1m two\u001b[0m legs (Score: 0.2302)\n",
      "2. Bird has\u001b[1m four\u001b[0m legs (Score: 0.2097)\n",
      "3. Bird has\u001b[1m 4\u001b[0m legs (Score: 0.1158)\n",
      "4. Bird has\u001b[1m six\u001b[0m legs (Score: 0.0539)\n",
      "5. Bird has\u001b[1m three\u001b[0m legs (Score: 0.0457)\n",
      "\n",
      "Top predictions:\n",
      "1. An averge human has\u001b[1m made\u001b[0m (Score: 0.0428)\n",
      "2. An averge human has\u001b[1m died\u001b[0m (Score: 0.0349)\n",
      "3. An averge human has\u001b[1m become\u001b[0m (Score: 0.0321)\n",
      "4. An averge human has\u001b[1m evolved\u001b[0m (Score: 0.0292)\n",
      "5. An averge human has\u001b[1m.\u001b[0m (Score: 0.0281)\n",
      "\n",
      "Top predictions:\n",
      "1. an average human has\u001b[1m this\u001b[0m number of legs (Score: 0.2152)\n",
      "2. an average human has\u001b[1m that\u001b[0m number of legs (Score: 0.1503)\n",
      "3. an\u001b[1m a\u001b[0mverage human has\u001b[1m a\u001b[0m number of legs (Score: 0.0721)\n",
      "4. an average human has\u001b[1m similar\u001b[0m number of legs (Score: 0.0641)\n",
      "5. an average human has\u001b[1m any\u001b[0m number of legs (Score: 0.0544)\n",
      "Error: The input must contain the '<mask>' token.\n",
      "Error: The input must contain the '<mask>' token.\n",
      "Exiting application. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "fill_mask_pipeline = pipeline(\n",
    "    task=\"fill-mask\",\n",
    "    model=\"FacebookAI/roberta-base\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0\n",
    ")\n",
    "while True:\n",
    "    # Prompt the user for input in a single, continuous loop\n",
    "    input_text = input(\"\\nEnter a sentence with a <mask> token: \")\n",
    "    \n",
    "    # Exit condition\n",
    "    if input_text.lower() in ['quit', 'exit']:\n",
    "        print(\"Exiting application. Goodbye!\")\n",
    "        break\n",
    "    \n",
    "    # Validate input\n",
    "    if \"<mask>\" not in input_text:\n",
    "        print(\"Error: The input must contain the '<mask>' token.\")\n",
    "        continue\n",
    "    \n",
    "    # Perform inference\n",
    "    try:\n",
    "        # The pipeline call expects the input_text as an argument.\n",
    "        # The error you saw was likely due to a syntax issue or a bug in a specific environment.\n",
    "        # The correct call is `fill_mask_pipeline(input_text)`.\n",
    "        results = fill_mask_pipeline(input_text)\n",
    "        \n",
    "        # Print the top predictions\n",
    "        print(\"\\nTop predictions:\")\n",
    "        for i, result in enumerate(results):\n",
    "            # Format the output for readability and highlight the predicted word\n",
    "            sequence = result['sequence']\n",
    "            score = result['score']\n",
    "            token_str = result['token_str']\n",
    "            \n",
    "            # Highlight the predicted word in bold using ANSI escape codes\n",
    "            formatted_sequence = sequence.replace(token_str, f\"\\033[1m{token_str}\\033[0m\")\n",
    "            print(f\"{i + 1}. {formatted_sequence} (Score: {score:.4f})\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during inference: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
